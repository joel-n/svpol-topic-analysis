{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b44c9f6",
   "metadata": {},
   "source": [
    "# NLP pipeline tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8755b",
   "metadata": {},
   "source": [
    "## 1. Lemmatizer test: stanza\n",
    "Performance test, speed and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62af4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "import stanza\n",
    "stanza.download('sv') # download Swedish model, run only once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc18ef",
   "metadata": {},
   "source": [
    "Compare 2 pipelines with different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c5d209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 19:47:44 INFO: Loading these models for language: sv (Swedish):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | talbanken |\n",
      "| pos       | talbanken |\n",
      "| lemma     | talbanken |\n",
      "| depparse  | talbanken |\n",
      "=========================\n",
      "\n",
      "2021-12-10 19:47:44 INFO: Use device: cpu\n",
      "2021-12-10 19:47:44 INFO: Loading: tokenize\n",
      "2021-12-10 19:47:44 INFO: Loading: pos\n",
      "2021-12-10 19:47:45 INFO: Loading: lemma\n",
      "2021-12-10 19:47:45 INFO: Loading: depparse\n",
      "2021-12-10 19:47:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp1 = stanza.Pipeline('sv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8abf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 19:47:45 INFO: Loading these models for language: sv (Swedish):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | talbanken |\n",
      "| lemma     | talbanken |\n",
      "=========================\n",
      "\n",
      "2021-12-10 19:47:45 INFO: Use device: cpu\n",
      "2021-12-10 19:47:45 INFO: Loading: tokenize\n",
      "2021-12-10 19:47:45 INFO: Loading: lemma\n",
      "2021-12-10 19:47:46 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp2 = stanza.Pipeline('sv', processors='tokenize,lemma', use_gpu=True, pos_batch_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0638c84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 19:47:46 INFO: Loading these models for language: sv (Swedish):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | talbanken |\n",
      "| pos       | talbanken |\n",
      "| lemma     | talbanken |\n",
      "=========================\n",
      "\n",
      "2021-12-10 19:47:46 INFO: Use device: cpu\n",
      "2021-12-10 19:47:46 INFO: Loading: tokenize\n",
      "2021-12-10 19:47:46 INFO: Loading: pos\n",
      "2021-12-10 19:47:46 INFO: Loading: lemma\n",
      "2021-12-10 19:47:46 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp3 = stanza.Pipeline('sv', processors='tokenize,pos,lemma', use_gpu=True, pos_batch_size=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fc87a",
   "metadata": {},
   "source": [
    "Load test data, downloaded from https://data.riksdagen.se/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c934243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      Svar på interpellationer  Herr talman! Jeff A...\n",
      "2      Herr talman! Lotta Finstorp har frågat mig vi...\n",
      "3      Herr talman! Jag tackar ministern för svaret....\n",
      "4      Herr talman! Jag tackar Lotta Finstorp för de...\n",
      "5      Herr talman! Jag tackar ministern. Det känns ...\n",
      "6      Herr talman! Tack för inlägget  Lotta Finstor...\n",
      "7      Herr talman! Att den enskilde rent fysiskt si...\n",
      "8      Herr talman! Tack  Lotta Finstorp  för ditt i...\n",
      "9      Herr talman! Maria Stockhaus har frågat mig v...\n",
      "10     Herr talman! Tack  utbildningsministern  för ...\n",
      "Name: anforandetext, dtype: object\n"
     ]
    }
   ],
   "source": [
    "speeches1718 = pd.read_csv('anforande-201718.csvt', sep=',').iloc[1:11,:]\n",
    "print(speeches1718.anforandetext)\n",
    "speech_joint = \"\\n\\n\".join(speeches1718.anforandetext.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07bf021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions for the pipelines\n",
    "def preprocess1(text):\n",
    "    doc = nlp1(text)\n",
    "    tokens = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            tokens.append(word.lemma)\n",
    "    return tokens\n",
    "\n",
    "def preprocess2(text):\n",
    "    doc = nlp2(text)\n",
    "    tokens = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            tokens.append(word.lemma)\n",
    "    return tokens\n",
    "\n",
    "def preprocess3(text):\n",
    "    doc = nlp3(text)\n",
    "    tokens = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            tokens.append(word.lemma)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b3a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = preprocess1(speeches1718.anforandetext[1])\n",
    "p2 = preprocess2(speeches1718.anforandetext[1])\n",
    "p3 = preprocess3(speeches1718.anforandetext[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc81666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herr her\n",
      "Jeff jeff\n",
      "Ahl ahl\n",
      "åtgärda åtgära\n",
      "Jeff jeff\n",
      "Ahl ahl\n",
      "sjukhusläkare Sjukhusläkaren\n",
      "understryka understryk\n",
      "artikel artikeln\n",
      "inkoma inkom\n",
      "inspektion inspektionen\n",
      "personalbrist personalbri\n",
      "dödsfall dödsfallen\n",
      "vidta vidtaga\n",
      "patientsäkerhetslag patientsäkerhetslagen\n",
      "minimera minimer\n",
      "vårdskada vårdskadar\n",
      "socialstyrelse Socialstyrelsen\n",
      "lansera lanserat\n",
      "myndighet myndigheten\n",
      "vårdskada vårdskadar\n",
      "en den\n",
      "kommun kommuner\n",
      "yrkesgrupp yrkesgrupper\n",
      "vårdplats vårdplatser\n",
      "vidta vidtaga\n",
      "avsätta avsatt\n",
      "öronmärka öronmärk\n",
      "barnmorska barnmorskar\n",
      "socialstyrelse Socialstyrelsen\n",
      "förstärka förstärk\n",
      "ge gen\n",
      "socialstyrelse Socialstyrelsen\n",
      "myndighet myndigheten\n",
      "vårdskada vårdskadar\n",
      "MERGEFORMAT meRGEFORMe\n",
      "rikta riktad\n",
      "bemanning bemanningen\n",
      "rätta rätt\n",
      "kompetensförsörjningen kompetensförsörjningsproblemen\n",
      "vårdplats vårdplatser\n",
      "vårdplats vårdplatser\n",
      "uppehålla uppehåll\n",
      "utskrivningsklara utskrivningsklar\n",
      "sliten slita\n",
      "vårdplats vårdplatser\n",
      "utskrivningsklara utskrivningsklar\n",
      "prioritera prioriter\n",
      "behov behova\n",
      "poängtera poängter\n",
      "en den\n",
      "åvila åvil\n",
      "vidta vidtar\n",
      "vårdplats vårdplatser\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 2 yields worse results due to the missing POS tagger\n",
    "for i in range(len(p1)):\n",
    "    if p1[i] != p2[i]:\n",
    "        print(p1[i], p2[i])\n",
    "\n",
    "print('------')\n",
    "\n",
    "# Pipeline 1 and 3 yield identical result on the second text\n",
    "for i in range(len(p1)):\n",
    "    if p3[i] != p3[i]:\n",
    "        print(p1[i], p3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce387723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1: 53.35 seconds.\n",
      "(10, 970)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "vectorizer1 = TfidfVectorizer(tokenizer=preprocess1)\n",
    "speech_vec1 = vectorizer1.fit_transform(speeches1718.anforandetext.values)\n",
    "t = time() - t\n",
    "print(f\"Pipeline 1: {round(t, 2)} seconds.\")\n",
    "print(speech_vec1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1652c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 2: 30.09 seconds.\n",
      "(10, 1008)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=preprocess2)\n",
    "speech_vec2 = vectorizer2.fit_transform(speeches1718.anforandetext.values)\n",
    "t = time() - t\n",
    "print(f\"Pipeline 2: {round(t, 2)} seconds.\")\n",
    "print(speech_vec2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24031802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 2 (with concatenated text): 9.16 seconds.\n",
      "(1, 1008)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=preprocess2)\n",
    "speech_vec21 = vectorizer2.fit_transform([speech_joint])\n",
    "t = time() - t\n",
    "print(f\"Pipeline 2 (with concatenated text): {round(t, 2)} seconds.\")\n",
    "\n",
    "# Problem with concatenating, nlp does not seem to recognize double\n",
    "# line breaks as in https://stanfordnlp.github.io/stanza/pipeline.html\n",
    "print(speech_vec21.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a89857",
   "metadata": {},
   "source": [
    "Using preprocessor outside of TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2247e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 2 (with stanza docs): 9.76 seconds.\n",
      "(10, 984)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "# Wrap each document with a stanza.Document object\n",
    "in_docs = [stanza.Document([], text=d) for d in speeches1718.anforandetext.values]\n",
    "out_docs = nlp2(in_docs)\n",
    "\n",
    "# print(out_docs)\n",
    "\n",
    "tokens = []\n",
    "for doc in out_docs:\n",
    "    tokens_ = []\n",
    "    for sentence in doc.sentences:\n",
    "        tokens_.extend([word.lemma for word in sentence.words])\n",
    "    tokens.append(' '.join(tokens_))\n",
    "\n",
    "vectorizer22 = TfidfVectorizer()\n",
    "speech_vec22 = vectorizer22.fit_transform(tokens)    \n",
    "    \n",
    "t = time() - t\n",
    "print(f\"Pipeline 2 (with stanza docs): {round(t, 2)} seconds.\")\n",
    "print(speech_vec22.shape) # OK!\n",
    "\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d95f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 1 (with stanza docs): 31.16 seconds.\n",
      "(10, 941)\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "out_docs1 = nlp1(in_docs)\n",
    "\n",
    "tokens = []\n",
    "for doc in out_docs1:\n",
    "    tokens_ = []\n",
    "    for sentence in doc.sentences:\n",
    "        tokens_.extend([word.lemma for word in sentence.words])\n",
    "    tokens.append(' '.join(tokens_))\n",
    "\n",
    "vectorizer12 = TfidfVectorizer()\n",
    "speech_vec12 = vectorizer12.fit_transform(tokens)    \n",
    "    \n",
    "t = time() - t\n",
    "print(f\"Pipeline 1 (with stanza docs): {round(t, 2)} seconds.\")\n",
    "print(speech_vec12.shape) # OK!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08cd8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method with stanza documents provides equally good results\n",
    "p12 = tokens[0].split(' ')\n",
    "for i in range(len(p1)):\n",
    "    if p1[i] != p12[i]:\n",
    "        print(p1[i], p12[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e82b4",
   "metadata": {},
   "source": [
    "It seems to be possible to cut processing time in almost half using POS tagger, and more than half if the POS tagger is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476fbc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vocab = vectorizer.vocabulary_\n",
    "# inv_vocab = {val:key for key, val in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ff427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
